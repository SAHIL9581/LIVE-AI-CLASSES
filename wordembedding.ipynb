{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMba2KL7ZKs3qmuLkvQO2DI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAHIL9581/LIVE-AI-CLASSES/blob/main/wordembedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "bmXYY5pscgXu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok34V7WPL4dE",
        "outputId": "689461fe-1cd6-4b93-dfb0-2098761ba8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word2Ind: {'hello': 0, 'learning': 1, 'machine': 2, 'world': 3}\n",
            "Ind2word: {0: 'hello', 1: 'learning', 2: 'machine', 3: 'world'}\n"
          ]
        }
      ],
      "source": [
        "def get_dict(words):\n",
        "    \"\"\"\n",
        "    Generate word-to-index and index-to-word dictionaries.\n",
        "\n",
        "    Args:\n",
        "        words (list of str): List of words from a tokenized corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (word2Ind, Ind2word) dictionaries.\n",
        "    \"\"\"\n",
        "    unique_words = sorted(set(words))  # Sort for consistency\n",
        "    word2Ind = {word: i for i, word in enumerate(unique_words)}\n",
        "    Ind2word = {i: word for word, i in word2Ind.items()}\n",
        "\n",
        "    return word2Ind, Ind2word\n",
        "\n",
        "# Example usage\n",
        "words = [\"hello\", \"world\", \"hello\", \"machine\", \"learning\"]\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "print(\"word2Ind:\", word2Ind)\n",
        "print(\"Ind2word:\", Ind2word)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenized version of the corpus\n",
        "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
        "\n",
        "# Define V. Remember this is the size of the vocabulary\n",
        "V = 5\n",
        "\n",
        "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "\n",
        "# Define first matrix of weights\n",
        "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
        "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
        "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
        "\n",
        "# Define second matrix of weights\n",
        "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
        "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
        "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
        "               [ 0.07055222, -0.02015138,  0.36107434],\n",
        "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
        "\n",
        "# Define first vector of biases\n",
        "b1 = np.array([[ 0.09688219],\n",
        "               [ 0.29239497],\n",
        "               [-0.27364426]])\n",
        "\n",
        "# Define second vector of biases\n",
        "b2 = np.array([[ 0.0352008 ],\n",
        "               [-0.36393384],\n",
        "               [-0.12775555],\n",
        "               [-0.34802326],\n",
        "               [-0.07017815]])"
      ],
      "metadata": {
        "id": "-htOXc0tMAYZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print W1\n",
        "W1"
      ],
      "metadata": {
        "id": "hRe4nLjtMBK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7537ad-cb75-43d6-e0d0-05360be460cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
              "       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
              "       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print corresponding word for each index within vocabulary's range\n",
        "for i in range(V):\n",
        "    print(Ind2word[i])"
      ],
      "metadata": {
        "id": "ehB6mizVME5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7659f6-7916-4934-e5d8-5dc06a5b5923"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am\n",
            "because\n",
            "happy\n",
            "i\n",
            "learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "    # Extract the column corresponding to the index of the word in the vocabulary\n",
        "    word_embedding_vector = W1[:, word2Ind[word]]\n",
        "    # Print word alongside word embedding vector\n",
        "    print(f'{word}: {word_embedding_vector}')"
      ],
      "metadata": {
        "id": "3wMW5HTbMFzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17051fc9-9e69-45d0-c744-096bce7218f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [0.41687358 0.32735501 0.26637602]\n",
            "because: [ 0.08854191  0.22795148 -0.23846886]\n",
            "happy: [-0.23495225 -0.23951958 -0.37770863]\n",
            "i: [ 0.28320538  0.4117634  -0.11399446]\n",
            "learning: [ 0.41800106 -0.23924344  0.34008124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute W3 as the average of W1 and W2 transposed\n",
        "W3 = (W1+W2.T)/2\n",
        "\n",
        "# Print W3\n",
        "W3"
      ],
      "metadata": {
        "id": "1oy0sQ9hMHhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46157e07-5cec-4fcf-ffff-32b0b1df9d99"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
              "       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
              "       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "    # Extract the column corresponding to the index of the word in the vocabulary\n",
        "    word_embedding_vector = W3[:, word2Ind[word]]\n",
        "    # Print word alongside word embedding vector\n",
        "    print(f'{word}: {word_embedding_vector}')"
      ],
      "metadata": {
        "id": "3WSD3caJMJbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c4c3ea-c83c-4220-eee5-194182504dc3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [ 0.09752647 -0.05136565  0.19974284]\n",
            "because: [ 0.08665397  0.15459171 -0.03063173]\n",
            "happy: [-0.02389858 -0.15029611 -0.27839106]\n",
            "i: [0.1768788  0.19580601 0.12353994]\n",
            "learning: [ 0.3764029  -0.31673866 -0.04975536]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the size of the word embedding vectors and save it in the variable 'N'\n",
        "N = 3\n",
        "\n",
        "# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\n",
        "V = 5\n"
      ],
      "metadata": {
        "id": "udW2wz_0MNXZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define first matrix of weights\n",
        "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
        "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
        "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
        "\n",
        "# Define second matrix of weights\n",
        "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
        "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
        "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
        "               [ 0.07055222, -0.02015138,  0.36107434],\n",
        "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
        "\n",
        "# Define first vector of biases\n",
        "b1 = np.array([[ 0.09688219],\n",
        "               [ 0.29239497],\n",
        "               [-0.27364426]])\n",
        "\n",
        "# Define second vector of biases\n",
        "b2 = np.array([[ 0.0352008 ],\n",
        "               [-0.36393384],\n",
        "               [-0.12775555],\n",
        "               [-0.34802326],\n",
        "               [-0.07017815]])"
      ],
      "metadata": {
        "id": "mm_FgsZyMPFO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'V (vocabulary size): {V}')\n",
        "print(f'N (embedding size / size of the hidden layer): {N}')\n",
        "print(f'size of W1: {W1.shape} (NxV)')\n",
        "print(f'size of b1: {b1.shape} (Nx1)')\n",
        "print(f'size of W2: {W2.shape} (VxN)')\n",
        "print(f'size of b2: {b2.shape} (Vx1)')"
      ],
      "metadata": {
        "id": "0BmcgkUPMS5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103d9e04-29c6-4c39-aa3f-5335c3027541"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V (vocabulary size): 5\n",
            "N (embedding size / size of the hidden layer): 3\n",
            "size of W1: (3, 5) (NxV)\n",
            "size of b1: (3, 1) (Nx1)\n",
            "size of W2: (5, 3) (VxN)\n",
            "size of b2: (5, 1) (Vx1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenized version of the corpus\n",
        "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
        "\n",
        "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "# Define the 'get_windows' function as seen in a previous notebook\n",
        "def get_windows(words, C):\n",
        "    i = C\n",
        "    while i < len(words) - C:\n",
        "        center_word = words[i]\n",
        "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
        "        yield context_words, center_word\n",
        "        i += 1\n",
        "\n",
        "# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\n",
        "def word_to_one_hot_vector(word, word2Ind, V):\n",
        "    one_hot_vector = np.zeros(V)\n",
        "    one_hot_vector[word2Ind[word]] = 1\n",
        "    return one_hot_vector\n",
        "\n",
        "# Define the 'context_words_to_vector' function as seen in a previous notebook\n",
        "def context_words_to_vector(context_words, word2Ind, V):\n",
        "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
        "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
        "    return context_words_vectors\n",
        "\n",
        "# Define the generator function 'get_training_example' as seen in a previous notebook\n",
        "def get_training_example(words, C, word2Ind, V):\n",
        "    for context_words, center_word in get_windows(words, C):\n",
        "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
      ],
      "metadata": {
        "id": "CA-FuSvfMTT7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save generator object in the 'training_examples' variable with the desired arguments\n",
        "training_examples = get_training_example(words, 2, word2Ind, V)"
      ],
      "metadata": {
        "id": "qwzDXOq_MV7Q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get first values from generator\n",
        "x_array, y_array = next(training_examples)"
      ],
      "metadata": {
        "id": "khaSJwa1MX8e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print context words vector\n",
        "x_array"
      ],
      "metadata": {
        "id": "LvQc3oC4MaAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc7c955-9da3-4784-d1a2-a4f6ac7be492"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print one hot vector of center word\n",
        "y_array"
      ],
      "metadata": {
        "id": "e6wQE9Z_MbfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37938e0-195c-4118-cdb1-6867b2778b91"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy vector\n",
        "x = x_array.copy()\n",
        "\n",
        "# Reshape it\n",
        "x.shape = (V, 1)\n",
        "\n",
        "# Print it\n",
        "print(f'x:\\n{x}\\n')\n",
        "\n",
        "# Copy vector\n",
        "y = y_array.copy()\n",
        "\n",
        "# Reshape it\n",
        "y.shape = (V, 1)\n",
        "\n",
        "# Print it\n",
        "print(f'y:\\n{y}')"
      ],
      "metadata": {
        "id": "oackRz0CMdX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323c86cf-40a6-405e-df77-1c7a2cf72028"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:\n",
            "[[0.25]\n",
            " [0.25]\n",
            " [0.  ]\n",
            " [0.5 ]\n",
            " [0.  ]]\n",
            "\n",
            "y:\n",
            "[[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the 'relu' function\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "# Define the 'softmax' function\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z))\n",
        "    return exp_z / np.sum(exp_z)\n"
      ],
      "metadata": {
        "id": "l2tXuBW4MfBR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute z1 (values of first hidden layer before applying the ReLU function)\n",
        "z1 = z1 = np.dot(W1, x) + b1\n",
        "\n",
        "print(\"z1:\", z1)"
      ],
      "metadata": {
        "id": "Ap76-OaQMgvd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8e2f39-8efe-49bb-cd67-00795eb77684"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z1: [[ 0.36483875]\n",
            " [ 0.63710329]\n",
            " [-0.3236647 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute h (z1 after applying ReLU function)\n",
        "h = relu(z1)\n",
        "\n",
        "# Print h\n",
        "h"
      ],
      "metadata": {
        "id": "7ZwN5wAcMjGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4084c52-5ea4-4c6a-b0c4-e8f04d07f386"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.36483875],\n",
              "       [0.63710329],\n",
              "       [0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z2 = np.dot(W2, h) + b2\n",
        "print(\"z2:\", z2)"
      ],
      "metadata": {
        "id": "JSbzTTrlMmO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d753ab2-34a2-4ff8-ad8a-dd013847a8d2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z2: [[-0.31973737]\n",
            " [-0.28125477]\n",
            " [-0.09838369]\n",
            " [-0.33512159]\n",
            " [-0.19919612]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = softmax(z2)\n",
        "\n",
        "# Print y_hat\n",
        "print(\"y_hat:\", y_hat)"
      ],
      "metadata": {
        "id": "AbsRBywiMoSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aaa3f3d-2d2c-4391-90ae-930db154d7d3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_hat: [[0.18519074]\n",
            " [0.19245626]\n",
            " [0.23107446]\n",
            " [0.18236353]\n",
            " [0.20891502]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print prediction\n",
        "prediction = np.argmax(y_hat)\n",
        "\n",
        "# Print prediction\n",
        "print(\"Predicted target:\", prediction)"
      ],
      "metadata": {
        "id": "Je6IoqR7MqPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8875339e-1985-46d5-de51-202a2cb2c688"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted target: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Target value (y):\", y)"
      ],
      "metadata": {
        "id": "pw1sw9KPMrs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de554847-8a9a-4bed-a5d7-45715e80a442"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target value (y): [[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y_predicted, y_actual):\n",
        "    epsilon = 1e-10\n",
        "    y_predicted = np.clip(y_predicted, epsilon, 1.0)  # Clip to prevent log(0)\n",
        "\n",
        "    # Compute cross-entropy loss\n",
        "    loss = -np.sum(y_actual * np.log(y_predicted))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "1Gd12nUHMtQz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print value of cross entropy loss for prediction and target value\n",
        "cross_entropy_loss(y_hat, y)"
      ],
      "metadata": {
        "id": "9pGNfdPNMuyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c81a01-7c83-4468-eff5-0fb1424266b2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4650152923611108"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grad_b2 = y_hat - y  # y_hat is the predicted probability distribution, y is the true one-hot encoded label\n",
        "\n",
        "# Print the gradient vector\n",
        "print(\"grad_b2:\", grad_b2)"
      ],
      "metadata": {
        "id": "ZIOU3gPDMw9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa82f75c-a82d-493a-dda9-4a788af5e38d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_b2: [[ 0.18519074]\n",
            " [ 0.19245626]\n",
            " [-0.76892554]\n",
            " [ 0.18236353]\n",
            " [ 0.20891502]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute grad_W2 using the outer product of (y_hat - y) and h (the activations of the hidden layer)\n",
        "grad_W2 = np.outer(y_hat - y, h)\n",
        "\n",
        "# Print the gradient matrix\n",
        "print(\"grad_W2:\\n\", grad_W2)"
      ],
      "metadata": {
        "id": "O9gv5Wa-M2UH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7a4f61-66bc-4a58-94dd-59c73cfa33e2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_W2:\n",
            " [[ 0.06756476  0.11798563  0.        ]\n",
            " [ 0.0702155   0.12261452  0.        ]\n",
            " [-0.28053384 -0.48988499 -0.        ]\n",
            " [ 0.06653328  0.1161844   0.        ]\n",
            " [ 0.07622029  0.13310045  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute vector with partial derivatives of loss function with respect to b1\n",
        "grad_b1 = relu(np.dot(W2.T, y_hat - y))\n",
        "\n",
        "# Print vector\n",
        "print(\"grad_b1:\", grad_b1)"
      ],
      "metadata": {
        "id": "HH2_gebZM3pr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac2899fb-1c6a-495a-ac3d-3e44b3c23549"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_b1: [[0.        ]\n",
            " [0.        ]\n",
            " [0.17045858]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute matrix with partial derivatives of loss function with respect to W1\n",
        "grad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n",
        "\n",
        "# Print matrix\n",
        "print(\"grad_W1:\\n\", grad_W1)"
      ],
      "metadata": {
        "id": "ghrSZ4M1M5Om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5901011-ff66-4c16-c1cf-fe9b65c15a89"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_W1:\n",
            " [[0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.04261464 0.04261464 0.         0.08522929 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'V (vocabulary size): {V}')\n",
        "print(f'N (embedding size / size of the hidden layer): {N}')\n",
        "print(f'size of grad_W1: {grad_W1.shape} (NxV)')\n",
        "print(f'size of grad_b1: {grad_b1.shape} (Nx1)')\n",
        "print(f'size of grad_W2: {grad_W2.shape} (VxN)')\n",
        "print(f'size of grad_b2: {grad_b2.shape} (Vx1)')"
      ],
      "metadata": {
        "id": "jLvj1O5sM8Dv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53054149-bc6b-4501-d1fc-db988fc1807f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V (vocabulary size): 5\n",
            "N (embedding size / size of the hidden layer): 3\n",
            "size of grad_W1: (3, 5) (NxV)\n",
            "size of grad_b1: (3, 1) (Nx1)\n",
            "size of grad_W2: (5, 3) (VxN)\n",
            "size of grad_b2: (5, 1) (Vx1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define alpha\n",
        "alpha = 0.03"
      ],
      "metadata": {
        "id": "ujeSnBx1M8jY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute updated W1\n",
        "eta = 0.02 # example\n",
        "W1_new = W1 - eta * grad_W1"
      ],
      "metadata": {
        "id": "30I5eYn1M-E2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('old value of W1:')\n",
        "print(W1)\n",
        "print()\n",
        "print('new value of W1:')\n",
        "print(W1_new)"
      ],
      "metadata": {
        "id": "9iu8qwFQM_kO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1872f9-a407-4e6b-b789-cec1c042c8ea"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "old value of W1:\n",
            "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
            " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
            " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n",
            "\n",
            "new value of W1:\n",
            "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
            " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
            " [ 0.26594987 -0.23889501 -0.37770863 -0.11484675  0.34008124]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update W2, b1, and b2\n",
        "eta = 0.01 # example\n",
        "W2_new = W2 - eta * grad_W2\n",
        "b1_new = b1 - eta * grad_b1\n",
        "b2_new = b2 - eta * grad_b2\n",
        "\n",
        "\n",
        "print('W2_new')\n",
        "print(W2_new)\n",
        "print()\n",
        "print('b1_new')\n",
        "print(b1_new)\n",
        "print()\n",
        "print('b2_new')\n",
        "print(b2_new)"
      ],
      "metadata": {
        "id": "bf_pvbiCNBgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0dd1a4f-8c61-47a6-be2d-6cbc4c9e9abc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W2_new\n",
            "[[-0.22249629 -0.43126617  0.13310965]\n",
            " [ 0.08406387  0.08000579  0.1772054 ]\n",
            " [ 0.18996044 -0.05617378 -0.1790735 ]\n",
            " [ 0.06988689 -0.02131322  0.36107434]\n",
            " [ 0.33404254 -0.39556489 -0.43959196]]\n",
            "\n",
            "b1_new\n",
            "[[ 0.09688219]\n",
            " [ 0.29239497]\n",
            " [-0.27534885]]\n",
            "\n",
            "b2_new\n",
            "[[ 0.03334889]\n",
            " [-0.3658584 ]\n",
            " [-0.12006629]\n",
            " [-0.3498469 ]\n",
            " [-0.0722673 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout"
      ],
      "metadata": {
        "id": "FeGnhdCGNC7L"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Implement Tokenization\n",
        "text = \"This is a simple example of text tokenization. Tokenization is important for NLP.\"\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences([text])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=10)"
      ],
      "metadata": {
        "id": "3SZ7MvhACAnT"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word Index:\", word_index)\n",
        "print(\"Sequences:\", sequences)\n",
        "print(\"Padded Sequences:\", padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "268nNFD4CI-O",
        "outputId": "89c0e16b-ebac-4430-a167-d5abef981df9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index: {'is': 1, 'tokenization': 2, 'this': 3, 'a': 4, 'simple': 5, 'example': 6, 'of': 7, 'text': 8, 'important': 9, 'for': 10, 'nlp': 11}\n",
            "Sequences: [[3, 1, 4, 5, 6, 7, 8, 2, 2, 1, 9, 10, 11]]\n",
            "Padded Sequences: [[ 5  6  7  8  2  2  1  9 10 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define Word Embeddings\n",
        "vocab_size = len(word_index) + 1"
      ],
      "metadata": {
        "id": "1B4IIHc4CLlh"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16 #dimension"
      ],
      "metadata": {
        "id": "CafmgSSXCPhz"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=10)"
      ],
      "metadata": {
        "id": "_PhcII8CCT0z"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_sequences = embedding_layer(padded_sequences)\n",
        "\n",
        "print(\"Embedded Sequences shape:\", embedded_sequences.shape)\n",
        "print(\"Embedded Sequences:\", embedded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BUQoFzeCYqr",
        "outputId": "a03548f9-56f6-413a-a23b-3aa9e8376dc2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded Sequences shape: (1, 10, 16)\n",
            "Embedded Sequences: tf.Tensor(\n",
            "[[[-3.2426216e-02  3.4641016e-02  4.9418453e-02 -5.5081137e-03\n",
            "   -3.7299536e-02  3.0639920e-02 -4.8702504e-02  2.5892463e-02\n",
            "    3.4027744e-02 -4.1327596e-02 -2.0136142e-02  3.9389480e-02\n",
            "    2.1365371e-02 -2.1932734e-02 -4.3475568e-02  4.9599554e-02]\n",
            "  [ 1.5686762e-02  4.2545583e-02 -2.8761148e-02 -2.0409977e-02\n",
            "    2.3830008e-02  4.9624894e-02 -3.1038094e-02 -1.9953383e-02\n",
            "   -4.4345092e-02 -1.9153321e-02  4.2004954e-02  3.3015694e-02\n",
            "   -1.5031528e-02  4.3350127e-02 -4.8912689e-04  3.0350748e-02]\n",
            "  [ 3.5907414e-02  4.0293224e-03  2.0123351e-02  4.2438354e-02\n",
            "   -1.9048179e-02  1.9976329e-02 -1.1644147e-02 -2.1293139e-02\n",
            "    2.7555499e-02  3.2838691e-02  1.2189411e-02  9.6194819e-04\n",
            "   -2.5600994e-02  2.9320780e-02  2.6475731e-02 -2.7577365e-02]\n",
            "  [ 3.4840349e-02 -4.1834425e-02  3.9689969e-02 -7.9488046e-03\n",
            "    3.3611406e-02 -2.6012436e-03 -3.0045286e-03  2.8687824e-02\n",
            "    9.9061802e-04  4.4446792e-02  4.5256410e-02  1.3216112e-02\n",
            "   -1.7913770e-02 -2.1615816e-02  2.6456241e-02 -1.5293669e-02]\n",
            "  [ 1.0233235e-02  2.4449434e-02  4.9136352e-02 -1.6759206e-02\n",
            "   -2.8499141e-03 -4.1868616e-02 -4.5842290e-02 -2.6693081e-02\n",
            "   -4.9851052e-03  4.3177512e-02 -1.5253164e-02 -9.8758563e-03\n",
            "    3.5127249e-02  4.7277097e-02  4.9765218e-02 -3.8413048e-02]\n",
            "  [ 1.0233235e-02  2.4449434e-02  4.9136352e-02 -1.6759206e-02\n",
            "   -2.8499141e-03 -4.1868616e-02 -4.5842290e-02 -2.6693081e-02\n",
            "   -4.9851052e-03  4.3177512e-02 -1.5253164e-02 -9.8758563e-03\n",
            "    3.5127249e-02  4.7277097e-02  4.9765218e-02 -3.8413048e-02]\n",
            "  [-4.2171814e-02  2.7673867e-02 -3.4071982e-02  1.4042344e-02\n",
            "    5.0553195e-03  4.9973238e-02  2.3897734e-02  2.8789509e-02\n",
            "   -2.5979985e-02 -4.4297934e-02  4.2405333e-02 -2.9594768e-02\n",
            "    4.2416561e-02  2.7358640e-02  4.6455834e-02 -1.8144391e-02]\n",
            "  [-3.8775481e-02  2.0697918e-02 -7.5124204e-05 -1.1066683e-03\n",
            "   -1.5561655e-04 -3.6472678e-02  2.6749101e-02 -3.6327291e-02\n",
            "   -2.2566522e-02  4.2898957e-02  4.1817427e-03  3.4621563e-02\n",
            "    2.1927882e-02  5.8943406e-03  3.6520127e-02  7.6170936e-03]\n",
            "  [-3.1897202e-02 -3.6009431e-02  3.3485841e-02 -3.5198443e-03\n",
            "   -4.0586244e-02  1.0831904e-02 -2.0824790e-02 -1.2854207e-02\n",
            "   -3.0230034e-02  3.6269676e-02 -3.8765918e-02  3.0676257e-02\n",
            "   -1.6620349e-02  2.8376888e-02 -3.8777985e-02 -3.8931917e-02]\n",
            "  [ 5.2892677e-03 -2.1714533e-02 -2.4565114e-02  3.3023391e-02\n",
            "    1.6381267e-02  6.9314949e-03 -4.8960626e-02  4.1312549e-02\n",
            "    1.4587890e-02 -2.7207209e-02 -3.2281123e-02  8.7250695e-03\n",
            "   -3.0824220e-02 -2.4975289e-02 -2.7213836e-02 -4.8362829e-02]]], shape=(1, 10, 16), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Build the CNN Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=10),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.5), # Add dropout for regularization\n",
        "    Dense(1, activation='sigmoid') # Binary classification example\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "XXg7LM8_ChWi"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = [\"good movie\", \"bad movie\"]\n",
        "train_labels = [1, 0]\n",
        "\n",
        "tokenizer_train = Tokenizer()\n",
        "tokenizer_train.fit_on_texts(train_texts)\n",
        "sequences_train = tokenizer_train.texts_to_sequences(train_texts)\n",
        "padded_train = pad_sequences(sequences_train, maxlen=10)\n",
        "\n",
        "# Convert train_labels to a NumPy array\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "model.fit(padded_train, train_labels, epochs=5, verbose=0)\n",
        "\n",
        "loss, accuracy = model.evaluate(padded_train, train_labels, verbose=0)\n",
        "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
        "\n",
        "new_texts = [\"very good movie\"]\n",
        "sequences_new = tokenizer_train.texts_to_sequences(new_texts)\n",
        "padded_new = pad_sequences(sequences_new, maxlen=10)\n",
        "predictions = model.predict(padded_new, verbose=0)\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6P4G-kYCuY9",
        "outputId": "9dc5414a-96eb-4e6e-fe98-3996d5ce8c38"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6849145889282227, Accuracy: 0.5\n",
            "Predictions: [[0.48656967]]\n"
          ]
        }
      ]
    }
  ]
}